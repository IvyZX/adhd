# This sentinel is a reminder to choose a real run name.
# If there is already a checkpoint under this run, that checkpoint will auto-resume.
run_name: ""

# If we aren't resuming from an existing checkpoint, load parameters from this path if provided.
load_parameters_path: ""

use_pjrt: "false"
reuse_example_batch: 0 # for testing TPU performance, this options repeated uses the same batch.

# Activation dtypes.
dtype: "bfloat16"
scale: 1
base_emb_dim: 512
base_num_heads: 8
base_mlp_dim: 2048
base_num_decoder_layers: 6
head_dim: 64
# activation functions are .
mlp_activations: ["relu"]
dropout_rate: 0.1
logits_via_embedding: False  # NOTE: this is True just for testing.
# minimal, full, or none
remat_policy: 'full'
scan_layers: True
param_scan_axis: 1

record_internal_nn_metrics: 0

# Output directory
base_output_directory: "gs://ivyzheng-us/maxtext"

# Parallelism
mesh_axes: ['data', 'model']
logical_axis_rules: [ ['batch', ['data', 'model']], ['vocab', 'model'], ['length', 'model'], ['heads', 'model'], ['mlp', 'model'], ['embed', 'model'],  ]
num_data_parallel_groups: 4

# Dataset
vocab_size: 30_000
vocab_path: "gs://ivyzheng-us/maxtext/vocabs/"  # Assumes we're allowed
dataset_name: 'lm1b'
eval_dataset_name: 'lm1b'
eval_split: 'test'
per_device_batch_size: 32
eval_per_device_batch_size: 32
max_corpus_chars: 10_000_000

# Training loop
steps: 500_001
log_period: 500
log_weight_histogram_period: 1000
learning_rate: 1.e-3
warmup_steps: 1000

# Checkpoint saving
save_period: 1000
save_max_to_keep: 10
save_keep_period: 10000

# Maximum length cutoff for training examples.
max_target_length: 128
# Maximum length cutoff for held-out evaluation examples.
max_eval_target_length: 512

# Maximum length cutoff for predicted tokens.
max_predict_length: 50
# Sampling temperature for language model inference.
sampling_temperature: 0.6
# Top k cutoff for logit sampling. If 0 then no top-k cutoff is used.
sampling_top_k: 20
eos_id: 2  # sentencepiece default
# Prompt for language model sampling.
prompt: "I love to "
